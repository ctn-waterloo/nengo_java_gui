"""
Unit tests for basic network logging
"""

import numpy as np
from ..nengo_deco import nengo_log

@nengo_log
def connect_something(logfile, orig=True, mode=None, seed=None):
    import nef
    import math
    net = nef.Network('top')
    netA = nef.Network('A')
    netB = nef.Network('B')
    netA.make_input('i0', math.sin)
    netA.make('X', 100, 1, mode=mode, seed=seed + 0)
    netA.make('Y', 100, 1, mode=mode, seed=seed + 1)
    netB.make('X', 100, 1, mode=mode, seed=seed + 2)
    netB.make('Y', 100, 1, mode=mode, seed=seed + 3)
    net.add(netA.network)
    net.add(netB.network)
    netA.connect('i0', 'X')
    net.connect('A.X', 'B.Y')
    #
    log = nef.log.TimelockedLog(network=net, filename=logfile)
    log.add('A.X')
    log.add('A.Y')
    log.add('B.X')
    log.add('B.Y')

    if orig:
        return net
    else:
        # XXX
        # None of the unit tests of logging need to test this branch
        # and this branch doesn't work.
        # TODO: make a new file full of unit tests that actually test the scripting mechanism.
        net.add_to_nengo()
        jython_src = net.network.dumpToScript()
        # -- autogenerated code names the toplevel thing 'net'
        exec jython_src
        return net


def test_sane_stats():
    stats0 = connect_something(t=1.0, seed=123, mode='spiking')
    assert np.allclose(stats0['time'], np.arange(0.0, 1.0, .001)[:, None])
    assert len(stats0['A.X']) == 1000
    assert len(stats0['A.Y']) == 1000
    assert len(stats0['B.X']) == 1000
    assert len(stats0['B.Y']) == 1000
    assert np.all(np.var(stats0['A.X']) > 0)


def diffstats(s0, s1, k):
    print "DiffStats", k
    t = np.hstack([s0[k],  s1[k]])
    for row in t:
        print row


def test_mode_has_some_effect():
    # test that a small model can be run twice and give the same result
    stats_spiking = connect_something(t=1.0, seed=123, mode='spiking')
    stats_rate= connect_something(t=1.0, seed=123, mode='rate')
    stats_direct = connect_something(t=1.0, seed=123, mode='direct')
    for key in sorted(stats_spiking.keys()):
        if key == 'time':
            assert np.all(stats_rate[key] == stats_spiking[key])
        else:
            # -- assert that most elements do not match
            assert np.mean(stats_rate[key] != stats_spiking[key]) > 0.9
            assert np.mean(stats_direct[key] != stats_spiking[key]) > 0.9
            # -- direct and rate modes actually match to 3 decimals pretty often
            assert np.mean(stats_direct[key] != stats_rate[key]) > 0.3


def test_reproducible_small_spiking(mode='spiking'):
    # test that a small model can be run twice and give the same result
    stats0 = connect_something(t=1.0, seed=123, mode=mode)
    stats1 = connect_something(t=1.0, seed=123, mode=mode)
    for key in sorted(stats0.keys()):
        try:
            assert np.all(stats0[key] == stats1[key])
        except:
            diffstats(stats0, stats1, key)
            raise


def test_reproducible_small_rate():
    return test_reproducible_small_spiking('rate')


def test_reproducible_small_direct():
    return test_reproducible_small_spiking('direct')


def test_reproducible_large():
    # XXX
    # -- simulate a large model with lots of chances for accidentally
    #    re-seeding things
    pass


def test_log_tau():
    # XXX
    pass


def test_seed_doesnt_matter_very_much():
    stats0 = connect_something(t=1.0, seed=123, mode='spiking')
    stats1 = connect_something(t=1.0, seed=124, mode='spiking')
    for key in stats0.keys():
        assert abs(np.mean(stats0[key]) - np.mean(stats1[key])) < 0.01, key
        assert abs(np.std(stats0[key]) - np.std(stats1[key])) < 0.01, key

